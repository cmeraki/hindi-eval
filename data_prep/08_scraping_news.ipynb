{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import requests\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from bs4 import BeautifulSoup\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rss_feed(url: str):\n",
    "    rss_xml = requests.get(url)\n",
    "\n",
    "    if rss_xml.status_code != 200:\n",
    "        print(f'Not able to query RSS feed page')\n",
    "        raise RuntimeError('Not able to query RSS feed page')\n",
    "\n",
    "    root = ET.fromstring(rss_xml.content)\n",
    "    items = root.findall('.//item')\n",
    "    titles = [i.find('./title').text.strip() for i in items]\n",
    "    links = [i.find('./link').text for i in items]\n",
    "\n",
    "    print(f'{len(links)} found')\n",
    "\n",
    "    return titles, links\n",
    "\n",
    "def get_individual_article(url: str, stop_phrases: List[str]):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f'Not able to query the article: {url}')\n",
    "        raise RuntimeError(f'Not able to query the article: {url}')\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    all_paras = [p.get_text() for p in soup.find_all('p')]\n",
    "\n",
    "    article_content = []\n",
    "    for para in all_paras:\n",
    "        for sp in stop_phrases:\n",
    "            if sp in para:\n",
    "                break\n",
    "        article_content.append(para)\n",
    "\n",
    "    # print(f'{len(all_paras)} paras found. {len(article_content)} paras parsed')\n",
    "\n",
    "    return '\\n\\n'.join(article_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_urls = [\n",
    "    'https://www.bhaskar.com/rss-v1--category-7140.xml',\n",
    "    'https://www.bhaskar.com/rss-v1--category-11215.xml',\n",
    "    'https://www.bhaskar.com/rss-v1--category-7911.xml',\n",
    "    'https://www.bhaskar.com/rss-v1--category-1051.xml',\n",
    "    'https://www.bhaskar.com/rss-v1--category-11616.xml'\n",
    "]\n",
    "phrases = ['पूरी खबर यहां पढ़ें...', 'ये खबर भी पढ़ें...', 'Copyright', 'पढ़ें पूरी खबर...', 'पूरी खबर पढ़ें...', 'This website follows the DNPA Code of Ethics']\n",
    "sleep_time = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for rss_url in rss_urls:\n",
    "\n",
    "    print(f'Processing for {rss_url}')\n",
    "    rss_titles, rss_feed = get_rss_feed(url=rss_url)\n",
    "\n",
    "    for t, l in tqdm(zip(rss_titles, rss_feed), total=len(rss_feed)):\n",
    "        try:\n",
    "            article = get_individual_article(url=l, stop_phrases=phrases)\n",
    "            dataset.append({\n",
    "                'link': l,\n",
    "                'title': t,\n",
    "                'content': article\n",
    "            })\n",
    "\n",
    "        except Exception as err:\n",
    "            continue\n",
    "        time.sleep(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in ds[4]['content'].split('\\n\\n'):\n",
    "    for p in phrases:\n",
    "        if p in elem:\n",
    "            print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in ds[4]['content'].split('\\n\\n'):\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.save_to_disk('../data/retreival/20231228-1604/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying and cleaning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List\n",
    "from datasets import load_from_disk, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_from_disk('../data/retrieval/20231228-1604/')\n",
    "phrases = ['पूरी खबर यहां पढ़ें...', 'ये खबर भी पढ़ें...', 'Copyright', 'पढ़ें पूरी खबर...', 'पूरी खबर पढ़ें...', 'This website follows the DNPA Code of Ethics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataset = []\n",
    "\n",
    "for datapoint in iter(ds):\n",
    "    content = datapoint['content']\n",
    "\n",
    "    joined_paras = []\n",
    "\n",
    "    for para in content.split('\\n\\n'):\n",
    "        # Remove paragraphs that are stop phrases or junk\n",
    "        flag = len([p for p in phrases if p in para]) >= 1\n",
    "        if flag:\n",
    "            continue\n",
    "        joined_paras.append(para)\n",
    "\n",
    "    num_paras = len(joined_paras)\n",
    "    if num_paras < 5:\n",
    "        continue\n",
    "\n",
    "    cleaned_dataset.append({\n",
    "        'link': datapoint['link'],\n",
    "        'content': datapoint['title'] + '\\n\\n' + '\\n\\n'.join(joined_paras),\n",
    "    })\n",
    "\n",
    "print(len(cleaned_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_ds = Dataset.from_list(cleaned_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = cleaned_ds.filter(lambda x: x['link'] == 'https://www.bhaskar.com/business/news/business-events-today-share-market-petrol-diesel-gold-silver-air-india-a350-132332950.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ds.filter(lambda x: x['link'] == 'https://www.bhaskar.com/business/news/business-events-today-share-market-petrol-diesel-gold-silver-air-india-a350-132332950.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a['content'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synth_save_to_disk(base_path: str, generated_dataset: List):\n",
    "    \"\"\"\n",
    "    Saves data to disk in a sequential file by removing elements from the list\n",
    "    The function will flush the contents of the List to the disk\n",
    "\n",
    "    Args:\n",
    "        base_path (str): Location of the base path where the data should be written\n",
    "        generated_dataset (List): Each element should be a valid Dict\n",
    "    \"\"\"\n",
    "\n",
    "    print(f'Number of rows: {len(generated_dataset)}')\n",
    "\n",
    "    with open(os.path.join(base_path, 'dataset.jsonl'), 'a', encoding='utf-8') as fp:\n",
    "        while generated_dataset:\n",
    "            fp.write(json.dumps(generated_dataset.pop(), ensure_ascii=False))\n",
    "            fp.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cleaned_dataset))\n",
    "synth_save_to_disk(base_path='../data/retrieval/cleaned_dataset/', generated_dataset=cleaned_dataset)\n",
    "print(len(cleaned_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/retrieval/cleaned_dataset/dataset.jsonl', 'r') as fp:\n",
    "    x = fp.read()\n",
    "    d = []\n",
    "\n",
    "    for ln in x.split('\\n'):\n",
    "        if not ln:\n",
    "            continue\n",
    "        d.append(json.loads(ln))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying gpt3.5 generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/synthetic_data/20231229-1514/retrieval_questions/dataset.jsonl', 'r') as fp:\n",
    "    x = fp.read()\n",
    "    data = []\n",
    "    for ln in x.split('\\n'):\n",
    "        if ln:\n",
    "            data.append(json.loads(ln))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mahout",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
