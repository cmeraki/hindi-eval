{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7e8f13-d3f5-4848-aea7-b3222bb8b7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import openai\n",
    "from textwrap import dedent\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_from_disk, Dataset\n",
    "import backoff\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1b9506-e6eb-4065-b927-cfa0b5f1e3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTGenerator():\n",
    "    def __init__(self, model_id) -> None:\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv()\n",
    "\n",
    "        self.client = openai.OpenAI()\n",
    "        self.model_id = model_id\n",
    "\n",
    "    @backoff.on_exception(backoff.expo, openai.RateLimitError, max_time=300)\n",
    "    def __call__(self, messages: List[str], temperature: float = 1.4) -> Tuple[Dict, any]:\n",
    "\n",
    "        completions = self.client.chat.completions.create(\n",
    "            model=self.model_id,\n",
    "            response_format={'type': 'json_object'},\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            # max_tokens=2048\n",
    "        )\n",
    "\n",
    "        if completions.choices[0].finish_reason == 'length':\n",
    "            raise IOError(f'Reached maximum output length, output format is not reliable. {completions.choices[0].message.content.strip()}')\n",
    "\n",
    "        op = json.loads(completions.choices[0].message.content)\n",
    "\n",
    "        # print(f'Prompts: {messages}, output: {op}')\n",
    "        # print(f'Tokens used in generation using {self.model_id}: {completions.usage}')\n",
    "\n",
    "        return op, completions.usage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387562ab-e520-4ad7-921d-ef69b677b690",
   "metadata": {},
   "source": [
    "## General MCQ\n",
    "- Load the dataset\n",
    "- Shuffle the options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd6e52a-1f06-42f3-ae72-ea94f57bc76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcq_data_path_part_1 = '../data/synthetic_data/20231228-1840/general_mcq/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176ba049-d150-4de4-81df-2496b51570cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcq_ds = load_from_disk(mcq_data_path_part_1)\n",
    "print(mcq_ds.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cca703-c878-4800-ad32-3b6c44855c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcq_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18b372d-dc76-486d-b6fd-2653148485c8",
   "metadata": {},
   "source": [
    "### Prompting\n",
    "- Sys prompt: Answer the MCQ question, output only option\n",
    "- User prompt: Question, options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86800dde-afe1-4dce-9552-0b765580934b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = {\n",
    "    'role': 'system',\n",
    "    'content': dedent('''\n",
    "    You are a helpful assistant who answers MCQ question.\n",
    "    The passage will be in Hindi/Hinglish. The answer needs to be in the same language of question.\n",
    "    Only output the single letter from (A/B/C/D) and always output in JSON. The fornat of the user query will be:\n",
    "\n",
    "    Question\n",
    "    \"\"\n",
    "\n",
    "    Choices\n",
    "    A.\n",
    "    B.\n",
    "    C.\n",
    "    D.\n",
    "\n",
    "    The correct answer is:\n",
    "    ''').strip()\n",
    "}\n",
    "\n",
    "usr_content = dedent('''\n",
    "    Question\n",
    "    {question}\n",
    "\n",
    "    Choices\n",
    "    A. {A}\n",
    "    B. {B}\n",
    "    C. {C}\n",
    "    D. {D}\n",
    "\n",
    "    The correct answer is:\n",
    "''').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e043ed-231d-487c-b2a9-0f9db60a07a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = GPTGenerator(model_id='gpt-4-1106-preview')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3dfe91-f6d7-47a1-94d1-a1e388ab8166",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mcq_validity = {\n",
    "#     'OUTPUT': [],\n",
    "#     'QUESTION': [],\n",
    "#     'PROMPT': []\n",
    "# }\n",
    "\n",
    "ip_tokens, op_tokens = 0, 0\n",
    "\n",
    "for i, elem in enumerate(tqdm(mcq_ds, total=mcq_ds.num_rows)):\n",
    "    if i <= 159:train_ds\n",
    "        continue\n",
    "    try:\n",
    "        usr_prompt = {\n",
    "            'role': 'user',\n",
    "            'content': usr_content.format(\n",
    "                question=elem['QUESTION'],\n",
    "                A=elem['A'],\n",
    "                B=elem['B'],\n",
    "                C=elem['C'],\n",
    "                D=elem['D']\n",
    "            )\n",
    "        }\n",
    "    \n",
    "        messages = [sys_prompt, usr_prompt]\n",
    "        output, tks = engine(messages, temperature=1)\n",
    "\n",
    "        mcq_validity['QUESTION'].append(elem['QUESTION'])\n",
    "        mcq_validity['PROMPT'].append(usr_prompt)\n",
    "        mcq_validity['OUTPUT'].extend(list(output.values()))\n",
    "    \n",
    "        ip_tokens += tks.prompt_tokens\n",
    "        op_tokens += tks.completion_tokens\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f'Input tokens: {ip_tokens}\\tOutput tokens: {op_tokens}')\n",
    "\n",
    "    except Exception as err:\n",
    "        print(f'Error: {err}')\n",
    "        continue\n",
    "\n",
    "    finally:\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2626db31-dea0-43d1-a1d4-f636f91659b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mcq_validity['OUTPUT']), len(mcq_validity['QUESTION']), len(mcq_validity['PROMPT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee47162-7d38-484b-9678-2e75ea885dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_tokens, op_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8962dfa1-6a5e-484a-b1e6-9efbf1b5340a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849a471b-d809-4f70-8447-d0a683f654f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a801843-845b-4dab-8ea7-5c73556f5782",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mcq_df = mcq_ds.to_pandas()\n",
    "\n",
    "mcq_validity = pd.DataFrame(mcq_validity)\n",
    "\n",
    "temp = mcq_validity.merge(mcq_df, on=['QUESTION'], how='inner')\n",
    "print(temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c897cf-6274-4d3e-8792-259eeb2a9aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.query('TARGET == OUTPUT').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d435882-6aef-49ab-93ae-f094efd51c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.query('TARGET != OUTPUT').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf3439a-a569-477b-8af2-32bc129632b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b360d6c-05d7-471c-b270-e419766bf13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e0d473-b912-4647-ac30-0bd921bc6407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'dataset' is your HuggingFace dataset\n",
    "def add_column_with_probability(dataset, new_column_name, value1, value2, probability=0.8):\n",
    "    new_values = [value1 if random.random() < probability else value2 for _ in range(len(dataset))]\n",
    "    dataset = dataset.add_column(new_column_name, new_values)\n",
    "    return dataset\n",
    "\n",
    "# Example usage\n",
    "export_ds = add_column_with_probability(mcq_ds, 'SPLIT', 'train', 'test', 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9504d2c6-cb69-444e-87e9-b47c5b612116",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = export_ds.filter(lambda example: example['SPLIT'] == 'train')\n",
    "train_ds = train_ds.filter(lambda example: example['TARGET'] in ['A', 'B', 'C', 'D'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59596e31-7258-4932-8af1-d69290e5889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = export_ds.filter(lambda example: example['SPLIT'] == 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aee701c-fa4d-4795-8cae-ff3633911b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.push_to_hub('cmeraki/eval_general_mcq', token='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0742e55-82f5-456a-9e94-f27f9e1a5e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds.push_to_hub('cmeraki/eval_general_mcq_private', token='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec8fac3-3421-4366-885e-806ac16603a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
