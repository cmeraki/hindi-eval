{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1489538-de09-4566-8224-db6d7798d418",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e94516a-b969-4837-be00-84e9175601c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk('../data/translation_eval_results/translate_evaluation_results/')\n",
    "df = dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bc735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dbfd90-c932-4732-80e6-526a38d9ecdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m, c1, c2, w in zip(df.loc[idx].messages, df.loc[idx].translated_reponse_m4t, df.loc[idx].translated_reponse_gpt, df.loc[idx].winners):\n",
    "    print(m['content'])\n",
    "    print('\\n')\n",
    "    print(f'm4t: {c1}')\n",
    "    print(f'cgpt: {c2}')\n",
    "    print(f'winner: {w}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0178d1-8ab5-4d43-b714-ab92f87e138e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'translated_reponse_m4t': 0,\n",
    "    'translated_reponse_gpt': 0\n",
    "}\n",
    "\n",
    "for elem in df.winners.to_list():\n",
    "    for e in elem:\n",
    "        results[e] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd6d2e2-4ecc-4182-a496-d4b69c703f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682d622b-6239-4413-93d3-563786ce8fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk('../data/tmp/2023122114/translation_eval/')\n",
    "df = dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daf8e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5986bfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(df.shape[0]):\n",
    "    for m, c1, c2, c3 in zip(df.loc[idx].messages, df.loc[idx].translated_reponse_m4t, df.loc[idx].translated_reponse_gpt, df.loc[idx].translated_reponse_gemini):\n",
    "        print(m['content'])\n",
    "        print('\\n')\n",
    "        print(f'm4t: {c1}')\n",
    "        print(f'cgpt: {c2}')\n",
    "        print(f'gemini: {c3}')\n",
    "        print('\\n')\n",
    "    print('\\n\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459a652a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_from_disk('../data/synthetic_data/20231227-2038/general_mcq/')\n",
    "print(ds.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d91605e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fc9d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "grade = ds.select_columns('GRADE').to_list()\n",
    "grade = [c['GRADE'] for c in grade]\n",
    "\n",
    "plt.hist(grade)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4c001f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = ds.select_columns('SUBJECT').to_list()\n",
    "sub = [c['SUBJECT'] for c in sub]\n",
    "\n",
    "plt.hist(sub)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0695fe5c",
   "metadata": {},
   "source": [
    "## notes\n",
    "\n",
    "I looked at 4 things to evaluate the synthetic data:\n",
    "1. Should follow instruction of generating question in either hindi or romanized hindi\n",
    "2. Answer should be correct\n",
    "3. Question should make logical sense\n",
    "4. No gibberish in the question/answer\n",
    "\n",
    "gpt3.5 performs very poor in generating synthetic data in hindi. Target can be either the answer text or option. Sometimes doesn't follow instructions to write in Hindi. Answer is not always correct and errors are high in chemistry and biology and 10th and 12th standard.\n",
    "\n",
    "\n",
    "gpt4 performs much better thant gpt3.5, but for biology and chemistry, the performance is not that great. So Ill reduce biology and chemistry topics and for maths and physics, will keep less 10th and 12th standard questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90719274",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import openai\n",
    "import random\n",
    "import json\n",
    "from textwrap import dedent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b64129",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7682380f",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = {\n",
    "    'role': 'system',\n",
    "    'content': dedent('''\n",
    "        You are an helpful assistant who verifies questions given in JSON format.\n",
    "        For every question, verify these things and reply:\n",
    "            1. Does the QUESTION make logical sense? Reply in one word, TRUE or FALSE\n",
    "            2. Is there any gibberish present in the QUESTION? Reply in one word, TRUE or FALSE\n",
    "            3. Is the answer in the TARGET field correct with respect to the question? Reply in one word, TRUE or FALSE\n",
    "            4. What language is the QUESTION in (Devnagri Hindi, Romanized Hindi, English or other)? Reply with the language name only\n",
    "\n",
    "        Always reply in JSON\n",
    "    ''').strip()\n",
    "}\n",
    "\n",
    "user_prompt = {\n",
    "    'role': 'user',\n",
    "    'content': None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9d5f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = ds.filter(lambda x: x['SUBJECT'] == 'Biology')\n",
    "sample = sample.select(\n",
    "    random.sample(range(sample.num_rows), 10)\n",
    ")\n",
    "\n",
    "print(sample.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675180d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ops = []\n",
    "\n",
    "for elem in iter(sample):\n",
    "    user_prompt.update({\n",
    "        'content': str(elem)\n",
    "    })\n",
    "\n",
    "    completions = client.chat.completions.create(\n",
    "        model='gpt-4-1106-preview',\n",
    "        response_format={'type': 'json_object'},\n",
    "        messages=[\n",
    "            system_prompt,\n",
    "            user_prompt\n",
    "        ],\n",
    "        max_tokens=1000\n",
    "    )\n",
    "    try:\n",
    "        op = json.loads(completions.choices[0].message.content)\n",
    "        op.update({'SUBJECT': elem['SUBJECT'], 'GRADE': elem['GRADE']})\n",
    "        print(op)\n",
    "        ops.append(op)\n",
    "    except:\n",
    "        raise ValueError('Value returned by the model is not valid JSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc92eb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "mental_maths = ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e299af",
   "metadata": {},
   "outputs": [],
   "source": [
    "chemistry_results = ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6ee95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "maths_results = ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d535f092",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyhsics_results = ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd03d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "biology_results = ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a07624f",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = maths_results\n",
    "illogical, gibberish, incorrect = 0, 0, 0\n",
    "grade = []\n",
    "\n",
    "for idx, e in enumerate(arr):\n",
    "    flag = 0\n",
    "    if e['1'] == 'FALSE':\n",
    "        illogical += 1\n",
    "        flag = 1\n",
    "    if e['2'] == 'TRUE':\n",
    "        gibberish += 1\n",
    "        flag = 1\n",
    "    if e['3'] == 'FALSE':\n",
    "        incorrect += 1\n",
    "        flag = 1\n",
    "\n",
    "    if flag:\n",
    "        grade.append(e['GRADE'])\n",
    "        print(e)\n",
    "        print(sample[idx])\n",
    "\n",
    "print(dedent(f\"\"\"\n",
    "Illogical: {illogical}\n",
    "Gibberish: {gibberish}\n",
    "Incorrect: {incorrect}\n",
    "Total: {len(arr)}\n",
    "\"\"\"))\n",
    "\n",
    "plt.hist(grade)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccd0b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in iter(sample):\n",
    "    print(elem)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21635e5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
